{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24167ff6",
   "metadata": {},
   "source": [
    "# Shay, Florrie & Sam's Data Science Project: MASTER NOTE BOOK\n",
    "AKA The Data Science Project team of dreams "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80ad40",
   "metadata": {},
   "source": [
    "# Introduction to the Project and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e013df36",
   "metadata": {},
   "source": [
    "\n",
    "This project will analyse data from the Old Bailey Online dataset, which is the largest collection of historical trial records from London's central criminal court, containing approximately 197,745 trials from 1674 to 1913. The Old Bailey Proceedings document detailed accounts of criminal trials, including information about defendants, victims, offences, verdicts, and punishments. We obtained the dataset from the Old Bailey API at https://www.oldbaileyonline.org/.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0fddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just importing some standard modules that we'll probably need.\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use ('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "pd.set_option ('mode.copy_on_write', True)\n",
    "\n",
    "# make sure you install the requirements.txt file to run this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43624d42",
   "metadata": {},
   "source": [
    "# Importing and Cleaning\n",
    "\n",
    " We parsed XML trial records and extracted key variables (defendants, victims, offences, verdicts, punishments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b5a8af",
   "metadata": {},
   "source": [
    "# Data Acquisition and Cleaning\n",
    "\n",
    "The Old Bailey's data is formatted through a variety of XML files. XML files are useful because they allow historical records to be stored in a structured, hierarchical format that preserves relationships between people and events and metadata. Which is important when you are trying to format your data in a way that you could look up certain statistics for the Old Bailey. Now, XML files are somewhat finicky unless you know how to clean them.\n",
    "\n",
    "Now, calling the below \"data cleaning\" may seem slightly disingenuous given we are selectively extracting particular variables, as opposed to correcting errors in the data itself. But, I digress. \n",
    "\n",
    "The first thing we need to do is get our imports and installs sorted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dde0463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\shayb\\onedrive - london interdisciplinary school\\university\\year 3\\modules\\term 1\\data science\\shaysenv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\shayb\\onedrive - london interdisciplinary school\\university\\year 3\\modules\\term 1\\data science\\shaysenv\\lib\\site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shayb\\onedrive - london interdisciplinary school\\university\\year 3\\modules\\term 1\\data science\\shaysenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shayb\\onedrive - london interdisciplinary school\\university\\year 3\\modules\\term 1\\data science\\shaysenv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shayb\\onedrive - london interdisciplinary school\\university\\year 3\\modules\\term 1\\data science\\shaysenv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shayb\\onedrive - london interdisciplinary school\\university\\year 3\\modules\\term 1\\data science\\shaysenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: word2number in c:\\users\\shayb\\onedrive - london interdisciplinary school\\university\\year 3\\modules\\term 1\\data science\\shaysenv\\lib\\site-packages (1.1)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'word2number'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mword2number\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m w2n\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'word2number'"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install word2number\n",
    "\n",
    "import xml.etree.ElementTree as ET # We need to import ElementTree to extract XML files, which is the file type the Old Bailey works with. \n",
    "# It will allow us to parse the XML files and directly pull their elements, as opposed to us using NLP (or something equally as troublesome)\n",
    "\n",
    "from pathlib import Path # Pathlib will be important because we are analysing a LOT of XML files, and we'll need to be doing checks throughout to see if all is well\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from collections import Counter\n",
    "from word2number import w2n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe8a96",
   "metadata": {},
   "source": [
    "Now, let's first just get all the files in one place and check we have all of them. Given the sheer quantity the Old Bailey database contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfdb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path('.') # Given how many files we are working with, we can't actually keep the files in the GitHub. \n",
    "# As such, we are using the pathlib function just in case things end up breaking down across computers.\n",
    "\n",
    "da_xml_files = list(input_dir.glob('*.xml')) \n",
    "# This will search the folder for all files ending with '.xml', thus finding all the files we need. \n",
    "# Listing them would be a bit problematic given how many there are. \n",
    "# It then creates a file called 'da_xml_files', which contains the name of all those files. \n",
    "\n",
    "print(f\"Found {len(da_xml_files)} files\") # This will double check how many files we have. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057dda2c",
   "metadata": {},
   "source": [
    "## The Function of Time\n",
    "\n",
    "Right, so, the most important piece of information we can extract from the XML files is the date of the various sessions. Without this, we won't be able to do much analysis at all as it all will be a function of time. So, we are going to create a function that extracts the session date and year from the tags within the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50839038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_i_could_put_time_in_a_dictionary(filepath):\n",
    "\n",
    "    tree = ET.parse(filepath) # This will create an ElementTree object, which is how python essentially \"sees\" XML files\n",
    "    root = tree.getroot() # This grabs the top-most tag for the XML files, and will help us find all the nested tags within it\n",
    "    \n",
    "    # Now we create empty strings for storage\n",
    "    session_date = \"\"\n",
    "    session_year = \"\"\n",
    "    \n",
    "    for div0 in root.iter('div0'): # The Old Bailey uses <div0> to mark top-level sections, like headers, cases, etc. So it's how we will split our data points by session as opposed to absorbing all of one XML file into a big mess\n",
    "\n",
    "        if div0.get('type') == 'sessionsPaper': # This checks if the <div0> element has the attribute which is describing a court session with date and time. Otherwise, we're not interested. \n",
    "\n",
    "            for interp in div0.iter('interp'): # In Old Bailey, <interp> holds those elements like date, time, etc\n",
    "                if interp.get('type') == 'date':\n",
    "                    session_date = interp.get('value', '')\n",
    "                elif interp.get('type') == 'year':\n",
    "                    session_year = interp.get('value', '') # Note that Old Bailey stores year and date separately. So, we need to store them separately\n",
    "            break\n",
    "    \n",
    "    if not session_year and len(session_date) >= 4: # Now, the data isn't perfect. So, if we are unable to extract the year, we can extract it from the title.\n",
    "        session_year = session_date[:4] # The first four numbers are the year for Old Bailey\n",
    "    \n",
    "    return {'date': session_date, 'year': session_year}\n",
    "\n",
    "# Now we run the function on our XML files, and print a couple to check its all worked! \n",
    "for xml_file in da_xml_files[:5]:\n",
    "    filepath = input_dir / xml_file\n",
    "    metadata = if_i_could_put_time_in_a_dictionary(filepath)\n",
    "    print(f\"{xml_file}: {metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567fd96c",
   "metadata": {},
   "source": [
    "## Getting Defendant Names (Like that one scene from Shawshank Redemption)\n",
    "\n",
    "Now, we need to extract the data on a defendant basis, and get a list of names/IDs. The reason we are doing this is because we are interested in the data being on a by-defendant basis, as opposed to a by-trial or session basis. The reason for this is some trials may have more than one defendant, or we have every row being on a per-day basis. Which doesn't really work if you want to get into demographic data.  \n",
    "\n",
    "Some information we want is their name, their id, and their gender. If we want to test any hypothesis relating to women, we will need this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfecb615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dammit_dufresne_you_are_putting_me_behind(trial_elem):\n",
    "    da_defendants = [] # Empty list to store defendant data\n",
    "    \n",
    "    # First, we'll create a loop that iterates over every XML name element within the trial element. \n",
    "    # Note that <persName> can be ANY name (defendant, witness, judge, etc), so we will need to do something about that\n",
    "    for person in trial_elem.iter('persName'): \n",
    "        if person.get('type') == 'defendantName': # This will ensure we are only extracting defendants specifically\n",
    "            def_id = person.get('id', '')\n",
    "            \n",
    "            given = \"\"\n",
    "            surname = \"\"\n",
    "            gender = \"\"\n",
    "            \n",
    "            # So, as we've established, the OldBailey XML files have tags for each defendant. \n",
    "            # It's actually quite amazing the work they've done because it is REALLY easy to extract if you know how\n",
    "            # So, we create a for loop that will iterate over every value attribute (what stores defendant data) and assign the correct name and gender. \n",
    "            for interp in person.iter('interp'):\n",
    "                inst = interp.get('inst')\n",
    "                if inst == def_id:\n",
    "                    interp_type = interp.get('type')\n",
    "                    if interp_type == 'given':\n",
    "                        given = interp.get('value', '')\n",
    "                    elif interp_type == 'surname':\n",
    "                        surname = interp.get('value', '')\n",
    "                    elif interp_type == 'gender':\n",
    "                        gender = interp.get('value', '')\n",
    "            \n",
    "            # Now, again, the Old Bailey data is HUGE and might not be perfect. \n",
    "            # So, just in case, we extract all the text content inside the <persName> element (which will be their name)\n",
    "            # And we just put it in their name. \n",
    "            # It's not going to be perfect, but realistically, the name doesn't matter as much so we don't mind. \n",
    "            if not given and not surname:\n",
    "                name_text = ''.join(person.itertext()).strip()\n",
    "                given = name_text\n",
    "            \n",
    "            # Now we just create our dictionary and BOOM. All done. \n",
    "            da_defendants.append({\n",
    "                'id': def_id,\n",
    "                'given': given,\n",
    "                'surname': surname,\n",
    "                'gender': gender\n",
    "            })\n",
    "    \n",
    "    return da_defendants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d20fc2",
   "metadata": {},
   "source": [
    "Now, let's run a quick test to make sure it all works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb6d8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a particular file (when we first did this, we picked five files at random. The below file was one of those five, hence why its specific).\n",
    "tree = ET.parse(input_dir / '18411129.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "count = 0 # We don't need to check all the trials within the file, so we'll set up a counter. \n",
    "for div1 in root.iter('div1'):\n",
    "    if div1.get('type') == 'trialAccount':\n",
    "        trial_id = div1.get('id')\n",
    "        defs = dammit_dufresne_you_are_putting_me_behind(div1)\n",
    "        print(f\"{trial_id}: {defs}\")\n",
    "        count += 1\n",
    "        if count >= 10: # End at 10 trials. Again, we don't need to waste time checking all of them. \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad6375b",
   "metadata": {},
   "source": [
    "Look there above. You can see some trials have more than one defendant. Thank god we decided to do this on a per-defendant basis, otherwise we would have been in trouble. Good luck Henry and Harriett with your trial (or rather, your t18411129)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d8be45",
   "metadata": {},
   "source": [
    "## What is the charge? Eating a meal? A succulent Chinese meal?\n",
    "\n",
    "Well, defendant data is all well and good. But another key factor is the actual crime they committed, otherwise, this isn't really good court data. Thankfully, we can extract that pretty easily as well! Though, there is some further explanation we need to do before proceeding. \n",
    "\n",
    "See, Old Bailey breaks down crime and offence through specific categorisation. There is the Category and the Subcategory. The category is the broad type of crime committed in the trial, so the likes of theft, assault, murder, fraud, etc. The subcategory handles a more specific classification within that broad category, so for example, theft could specifically be burglary, pickpocketing, or shoplifting. Now, one thing to note is that crime definition changes over time, even over the time of decades. Hence why it is actually hard to compare temporal crime statistics (and why murder is usually very good as a baseline, because its definition rarely changes. Either you've been murdered or you haven't). \n",
    "\n",
    "It should be noted that some defendants don't have subcategories on their crimes, and that's fine. It's not strictly vital, and it's something that can be filtered later if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc8fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right, so most of this stuff is the same again, so I won't go explaining everything. \n",
    "def i_am_offended(trial_elem):\n",
    "    get_off_fences = [] # Offence dictionary storage\n",
    "    \n",
    "    for rs in trial_elem.iter('rs'): # Note we are using <rs> here and not <persName> because Old Bailey stores offence data inside <rs> elements\n",
    "        # Specifically with type=\"offenceDescription\"\n",
    "        if rs.get('type') == 'offenceDescription':\n",
    "            off_id = rs.get('id', '')\n",
    "            category = \"\"\n",
    "            subcategory = \"\"\n",
    "            \n",
    "            for interp in rs.iter('interp'):\n",
    "                interp_type = interp.get('type')\n",
    "                if interp_type == 'offenceCategory':\n",
    "                    category = interp.get('value', '')\n",
    "                elif interp_type == 'offenceSubcategory':\n",
    "                    subcategory = interp.get('value', '')\n",
    "            \n",
    "            get_off_fences.append({\n",
    "                'id': off_id,\n",
    "                'category': category,\n",
    "                'subcategory': subcategory\n",
    "            })\n",
    "    \n",
    "    return get_off_fences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3acdfe6",
   "metadata": {},
   "source": [
    "And there you have it. Now, we do the test once again to just check everything is working fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33516328",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(input_dir / '18411129.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "count = 0\n",
    "for div1 in root.iter('div1'):\n",
    "    if div1.get('type') == 'trialAccount':\n",
    "        trial_id = div1.get('id')\n",
    "        offs = i_am_offended(div1)  \n",
    "        print(f\"{trial_id}: {offs}\")\n",
    "        \n",
    "        count += 1\n",
    "        if count >= 10:  \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e473c28",
   "metadata": {},
   "source": [
    "As a quick side. Theft Receiving means that the defendant knowingly accepted or bought goods that had been stolen; which is still a crime. Anyway, not important (truthfully speaking, I didn't know what it was and had to look it up. And I'm explaining it just in case).\n",
    "\n",
    "We move on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f86cf4",
   "metadata": {},
   "source": [
    "## 1000 counts of murder? To Arkham Asylum for the hundredth time with you! \n",
    "\n",
    "Right, and of course, we need the actual verdict and punishment, otherwise we have no value to the offences assigned to each defendant. After all, if they were all found innocent, then the Old Bailey dataset would be quite odd indeed. \n",
    "\n",
    "Once again, note there are subcategories. So, you can have guilty, not guilty as categories, and \"guilty of theft\" or \"guilty of assault\" as subcategories. Regarding punishment, categories work in the form of imprisonment -> 6 month imprisonment. So, the subcategories are actually important here and we will need to properly extract them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aedd9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, same old stuff. \n",
    "def we_the_jury_find_this_code_to_be_awful(trial_elem):\n",
    "    john_verdicts = []\n",
    "    \n",
    "    for rs in trial_elem.iter('rs'):\n",
    "        if rs.get('type') == 'verdictDescription':\n",
    "            ver_id = rs.get('id', '')\n",
    "            category = \"\"\n",
    "            subcategory = \"\"\n",
    "            \n",
    "            for interp in rs.iter('interp'):\n",
    "                interp_type = interp.get('type')\n",
    "                if interp_type == 'verdictCategory':\n",
    "                    category = interp.get('value', '')\n",
    "                elif interp_type == 'verdictSubcategory':\n",
    "                    subcategory = interp.get('value', '')\n",
    "            \n",
    "            john_verdicts.append({\n",
    "                'id': ver_id,\n",
    "                'category': category,\n",
    "                'subcategory': subcategory\n",
    "            })\n",
    "    \n",
    "    return john_verdicts\n",
    "\n",
    "def who_even_is_hammurabi_brah(trial_elem):\n",
    "    arya_starks_list = [] # Storage for defendant punishments\n",
    "    \n",
    "    for rs in trial_elem.iter('rs'):\n",
    "        if rs.get('type') == 'punishmentDescription':\n",
    "            pun_id = rs.get('id', '')\n",
    "            category = \"\"\n",
    "            subcategory = \"\"\n",
    "            \n",
    "            for interp in rs.iter('interp'):\n",
    "                interp_type = interp.get('type')\n",
    "                if interp_type == 'punishmentCategory':\n",
    "                    category = interp.get('value', '')\n",
    "                elif interp_type == 'punishmentSubcategory':\n",
    "                    subcategory = interp.get('value', '')\n",
    "            \n",
    "            arya_starks_list.append({\n",
    "                'id': pun_id,\n",
    "                'category': category,\n",
    "                'subcategory': subcategory\n",
    "            })\n",
    "    \n",
    "    return arya_starks_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc7d8f",
   "metadata": {},
   "source": [
    "And once again, a quick test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f53b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(input_dir / '18411129.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "count = 0\n",
    "for div1 in root.iter('div1'):\n",
    "    if div1.get('type') == 'trialAccount':\n",
    "        trial_id = div1.get('id')\n",
    "        verd = we_the_jury_find_this_code_to_be_awful(div1)\n",
    "        pun = who_even_is_hammurabi_brah(div1)\n",
    "        \n",
    "        print(f\"{trial_id}:\")\n",
    "        print(f\"  Verdicts: {verd}\")\n",
    "        print(f\"  Punishments: {pun}\")\n",
    "        \n",
    "        count += 1\n",
    "        if count >= 10: \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c917c",
   "metadata": {},
   "source": [
    "## Victim? I hardly know him.\n",
    "\n",
    "Right, the next thing we are going to do is extract details of the victims of each trial, similarly to how we did the defendants. This is mostly because it could yield interesting data regarding what crimes were committed on whom. After all, say a hypothesis regarding women. It's important to know the crimes being committed AGAINST women as well as by them if we want to get the full picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c9651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_victims_with_consent(trial_elem):\n",
    "    whomst_wronged = []\n",
    "    \n",
    "    for person in trial_elem.iter('persName'):\n",
    "        if person.get('type') == 'victimName':\n",
    "            vic_id = person.get('id', '')\n",
    "            \n",
    "            given = \"\"\n",
    "            surname = \"\"\n",
    "            gender = \"\"\n",
    "            \n",
    "            for interp in person.iter('interp'):\n",
    "                inst = interp.get('inst')\n",
    "                if inst == vic_id:\n",
    "                    interp_type = interp.get('type')\n",
    "                    if interp_type == 'given':\n",
    "                        given = interp.get('value', '')\n",
    "                    elif interp_type == 'surname':\n",
    "                        surname = interp.get('value', '')\n",
    "                    elif interp_type == 'gender':\n",
    "                        gender = interp.get('value', '')\n",
    "            \n",
    "            if not given and not surname: # As we did before with defendant names. This isn't always going to perfect. But we don't necessarily need it to be. \n",
    "                name_text = ''.join(person.itertext()).strip()\n",
    "                given = name_text\n",
    "            \n",
    "            whomst_wronged.append({\n",
    "                'id': vic_id,\n",
    "                'given': given,\n",
    "                'surname': surname,\n",
    "                'gender': gender\n",
    "            })\n",
    "    \n",
    "    return whomst_wronged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3269ad",
   "metadata": {},
   "source": [
    "You know what comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(input_dir / '18411129.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "count = 0\n",
    "for div1 in root.iter('div1'):\n",
    "    if div1.get('type') == 'trialAccount':\n",
    "        trial_id = div1.get('id')\n",
    "        vics = get_victims_with_consent(div1)\n",
    "        \n",
    "        print(f\"{trial_id}: Victims: {vics}\")\n",
    "        \n",
    "        count += 1\n",
    "        if count >= 10:  \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c7d6c",
   "metadata": {},
   "source": [
    "## Making Sense of the Madness\n",
    "\n",
    "Now, where do we go from here? Well, we have huge lists with defendants and their crimes, victims, gender, the like. But the issue is that currently, they are all scattered about isolated. Each in their own list. What we need is code that will join this information together. We need to tell which defendant committed which offence, received which punishment, or which victims were affected. This is the process that will actually let us turn the scattered, raw XML into a dataset; in which each row can correspond to one instance of a defendant, potentially even for each of his/her/their crimes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd98e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_together_wholesome(trial_elem):\n",
    "    # First off, lets create a dictionary to store all the information. A super dictionary if you will. \n",
    "    wholesome_joins = {\n",
    "        'criminalCharge': [],       # This will link the defendant to the offence and its verdict\n",
    "        'defendantPunishment': [],  # This will link the defendant to the punishment\n",
    "        'offenceVictim': [],        # This will link the defendant's offence to their victim\n",
    "        'offencePlace': [],         # This will link the defendant's offence to the location in which it occurred\n",
    "        'offenceCrimeDate': []      # # This will link the defendant's offence to the date it took place\n",
    "    }\n",
    "    \n",
    "    for join in trial_elem.iter('join'): # This will loop over all <join> elements. These elements are in the Old Bailey XML\n",
    "        # Basically, the Old Bailey team created <join> elements that link two entities together\n",
    "        result = join.get('result', '')\n",
    "        if result in wholesome_joins: # This will make sure to only keep joins that match the keys in the joins dictionary\n",
    "            # Basically, we don't want to pair random cases together\n",
    "            targets = join.get('targets', '').split()\n",
    "            wholesome_joins[result].append({\n",
    "                'id': join.get('id', ''),\n",
    "                'targets': targets\n",
    "            })\n",
    "    \n",
    "    return wholesome_joins\n",
    "\n",
    "\n",
    "# Let's test this to actually make sure that it works. \n",
    "# We'll use the Dell case, which has a multi-defendant instance with multiple charges.\n",
    "# If it works on this one, it'll work on all of them. \n",
    "\n",
    "tree = ET.parse(input_dir / '16791210.xml') # This is the file the Dell case is in. \n",
    "root = tree.getroot()\n",
    "\n",
    "for div1 in root.iter('div1'):\n",
    "    if div1.get('id') == 't16791210-10': # And this is the ID of the Dell case. \n",
    "        joins = join_together_wholesome(div1)\n",
    "        print(\"Criminal charges (defendant -> offence -> verdict):\")\n",
    "        for j in joins['criminalCharge']:\n",
    "            print(f\"  {j['targets']}\")\n",
    "        print(\"\\nDefendant punishments:\")\n",
    "        for j in joins['defendantPunishment']:\n",
    "            print(f\"  {j['targets']}\")\n",
    "        print(\"\\nOffence victims:\")\n",
    "        for j in joins['offenceVictim']:\n",
    "            print(f\"  {j['targets']}\")\n",
    "\n",
    "# Let's see if it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1fc228",
   "metadata": {},
   "source": [
    "And there you have it. It worked. You can see above that each list shows a link of defendant ID, offence ID, and verdict ID. This means the function successfully captured which defendant actually committed which offence and recieved what verdict. The following list has the punishment as well.\n",
    "\n",
    "Now, one interesting thing to note is the offence victim list is empty. That is odd. It's likely the case that this particular case had no listed victims. That said, we should still look into it. \n",
    "\n",
    "Let's search for trials that specifically HAVE an offence-victim relationship (so a victim who is identified), and then see if the code works on that. If not, then we'll know something is wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39368954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll search the same file to see which cases have victim IDs linked to offences. \n",
    "tree = ET.parse(input_dir / '16791210.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "print(\"1679 file - offenceVictim joins:\")\n",
    "for div1 in root.iter('div1'):\n",
    "    if div1.get('type') == 'trialAccount':\n",
    "        joins = join_together_wholesome(div1)\n",
    "        if joins['offenceVictim']:\n",
    "            print(f\"  {div1.get('id')}: {joins['offenceVictim']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520196bb",
   "metadata": {},
   "source": [
    "And there you have it. All sorted. It's just that the Dell case didn't have any victims listed. \n",
    "\n",
    "As such, we can move on from here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc53f8",
   "metadata": {},
   "source": [
    "## Crime Information Innit\n",
    "\n",
    "I can't think of a funny, cultural reference to go with this section. Sorry.\n",
    "\n",
    "Anyway, let's now make some functions to extract more information about each crime. As well as some more important information about the defendants, such as their age and occupation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b4249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of Crime\n",
    "def WHERE_is_she(trial_elem):\n",
    "    where_it_happenin = []\n",
    "    \n",
    "    for place in trial_elem.iter('placeName'):\n",
    "        place_id = place.get('id', '')  \n",
    "        place_name = ''.join(place.itertext()).strip()  \n",
    "        place_type = \"\"\n",
    "        \n",
    "        for interp in place.iter('interp'):\n",
    "            if interp.get('type') == 'type':\n",
    "                place_type = interp.get('value', '')\n",
    "        \n",
    "        if place_type == 'crimeLocation':\n",
    "            where_it_happenin.append({\n",
    "                'id': place_id,\n",
    "                'name': place_name\n",
    "            })\n",
    "    \n",
    "    return where_it_happenin\n",
    "\n",
    "# Date of Crime\n",
    "def omg_crime_has_got_a_date(trial_elem):\n",
    "    when_it_happenin = []\n",
    "    \n",
    "    for rs in trial_elem.iter('rs'):\n",
    "        if rs.get('type') == 'crimeDate':\n",
    "            when_it_happenin.append({\n",
    "                'id': rs.get('id', ''),\n",
    "                'date': ''.join(rs.itertext()).strip()\n",
    "            })\n",
    "    \n",
    "    return when_it_happenin\n",
    "\n",
    "\n",
    "# Onto Defendant Occupation\n",
    "# I am going to do a brief explanation for this one as functionally, it is different to the previous ones\n",
    "# Our previous functions worked off a \"give me everything related to X within a certain trial\"\n",
    "# This code below functions moreso as \"GIVEN this defendant, what is their occupation?\"\n",
    "# The key is the <join> element. As before, it is searching for related pieces of information that are connected via <join>\n",
    "\n",
    "def get_a_job(trial_elem, defendant_id):\n",
    "    for join in trial_elem.iter('join'):\n",
    "        if join.get('result') == 'persNameOccupation': # This makes sure all other joins except occupation are ignored\n",
    "            targets = join.get('targets', '').split() # Get the IDs the <join> links together\n",
    "            if defendant_id in targets: # Checks if the defendant is relevant to the relationship. If not, skip.\n",
    "                for target in targets: # Loop over each linked ID (defendant + occupation)\n",
    "                    for rs in trial_elem.iter('rs'): # Look through all <rs> elements in the trial\n",
    "                        # The reason we are doing this is because the <join> tag only tells us which IDs are related\n",
    "                        # It doesn't tell is where the actual text is that says what their job is\n",
    "                        if rs.get('id') == target and rs.get('type') == 'occupation': # That's what this code does below\n",
    "                            return ''.join(rs.itertext()).strip()\n",
    "    return \"\"\n",
    "\n",
    "# Defendant Age\n",
    "# This is similar to occupation\n",
    "def how_old_are_you(trial_elem, defendant_id):\n",
    "    \"\"\"Find age for a specific defendant.\"\"\"\n",
    "    for person in trial_elem.iter('persName'):\n",
    "        if person.get('id') == defendant_id:\n",
    "            for interp in person.iter('interp'):\n",
    "                if interp.get('type') == 'age':\n",
    "                    return interp.get('value', '')\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06626453",
   "metadata": {},
   "source": [
    "Let's do some quick tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(input_dir / '16931012.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "count = 0\n",
    "for div1 in root.iter('div1'):\n",
    "    if div1.get('type') == 'trialAccount':\n",
    "        trial_id = div1.get('id')\n",
    "        locs = WHERE_is_she(div1)\n",
    "        dates = omg_crime_has_got_a_date(div1)\n",
    "        \n",
    "        if locs or dates:  # Makes sure we only print trials at least one location or date\n",
    "            print(f\"{trial_id}:\")\n",
    "            if locs:\n",
    "                print(f\"  Locations: {locs}\")\n",
    "            if dates:\n",
    "                print(f\"  Dates: {dates}\")\n",
    "            count += 1\n",
    "            if count >= 10:  \n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc0280",
   "metadata": {},
   "source": [
    "Now hold on, the location doesn't appear to be extracted by our code. Let's try a test on all the XML files to see if its an issue that that particular file simply doesn't have locations within it. The code below will find the first XML file that has a viable location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3c4bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "found = False\n",
    "\n",
    "for xml_file in da_xml_files:\n",
    "    filepath = input_dir / xml_file\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for div1 in root.iter('div1'):\n",
    "        if div1.get('type') == 'trialAccount':\n",
    "            locations = WHERE_is_she(div1)\n",
    "            if locations:\n",
    "                print(f\"Found location in {xml_file}, trial {div1.get('id')}: {locations}\")\n",
    "                found = True\n",
    "                break  # Stop after first location in this file\n",
    "    if found:\n",
    "        break  # Stop after finding first location in all files\n",
    "\n",
    "if not found:\n",
    "    print(\"No crime locations found in any file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c67c1c9",
   "metadata": {},
   "source": [
    "Stoke Newington of all places. God forbid the land of the gentrified today experiences such crimes. Anyway, let's test that file specifically to see if it all works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8effbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(input_dir / '17151012.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "count = 0\n",
    "for div1 in root.iter('div1'):\n",
    "    if div1.get('type') == 'trialAccount':\n",
    "        trial_id = div1.get('id')\n",
    "        locs = WHERE_is_she(div1)\n",
    "        dates = omg_crime_has_got_a_date(div1)\n",
    "        \n",
    "        if locs and dates:  # We'll briefly change this to 'and' so we can actually pull up good ones. \n",
    "            print(f\"{trial_id}:\")\n",
    "            if locs:\n",
    "                print(f\"  Locations: {locs}\")\n",
    "            if dates:\n",
    "                print(f\"  Dates: {dates}\")\n",
    "            count += 1\n",
    "            if count >= 50:  \n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a70aac",
   "metadata": {},
   "source": [
    "## David Lammy would have this section not exist\n",
    "\n",
    "Right, now we are going to extract our last direct piece of information (there is one more thing that is less direct but we'll go into that later). Judge and Jury information. This is important because if we potentially want to run any hypothesis about bias in the judicial system historically, you will need to see patterns will specific jurors and judges. \n",
    "\n",
    "So, we are going to extract those on a per-offence basis, and it will be quite interesting to see later if any judges have any particular predilection towards a certain punishment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a46dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def its_too_late_i_have_already_depicted_you_as_the_buffoonish_juror_three_and_myself_as_the_unbothered_and_collected_juror_twelve(root): # I am not sorry for referencing 12 Angry Men. Good film. You should watch it\n",
    "    # I am maybe alightly sorry about the length of that function name lol\n",
    "    chewbacca_defence = []\n",
    "    for person in root.iter('persName'):\n",
    "        if person.get('type') == 'jurorName':\n",
    "            juror_id = person.get('id', '')\n",
    "            given = \"\"\n",
    "            surname = \"\"\n",
    "            for interp in person.iter('interp'):\n",
    "                if interp.get('type') == 'given':\n",
    "                    given = interp.get('value', '')\n",
    "                elif interp.get('type') == 'surname':\n",
    "                    surname = interp.get('value', '')\n",
    "            chewbacca_defence.append({\n",
    "                'id': juror_id,\n",
    "                'name': f\"{given} {surname}\".strip()\n",
    "            })\n",
    "    return chewbacca_defence\n",
    "\n",
    "def someones_judgy(root):\n",
    "    fudges = []\n",
    "    for person in root.iter('persName'):\n",
    "        if person.get('type') == 'judiciaryName':\n",
    "            judge_id = person.get('id', '')\n",
    "            given = \"\"\n",
    "            surname = \"\"\n",
    "            for interp in person.iter('interp'):\n",
    "                if interp.get('type') == 'given':\n",
    "                    given = interp.get('value', '')\n",
    "                elif interp.get('type') == 'surname':\n",
    "                    surname = interp.get('value', '')\n",
    "            fudges.append({\n",
    "                'id': judge_id,\n",
    "                'name': f\"{given} {surname}\".strip()\n",
    "            })\n",
    "    return fudges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb538c60",
   "metadata": {},
   "source": [
    "Test. Test. Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1a424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(input_dir / '17840707.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "jurors = its_too_late_i_have_already_depicted_you_as_the_buffoonish_juror_three_and_myself_as_the_unbothered_and_collected_juror_twelve(root)\n",
    "judges = someones_judgy(root)\n",
    "\n",
    "print(f\"Jurors ({len(jurors)}):\")\n",
    "for j in jurors[:5]:\n",
    "    print(f\"  {j}\")\n",
    "\n",
    "print(f\"\\nJudges ({len(judges)}):\")\n",
    "for j in judges[:5]:\n",
    "    print(f\"  {j}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0ea6d",
   "metadata": {},
   "source": [
    "## You see, four Farthings make a Halfpenny, and two Halfpennies make one Penny, and four Pence make a Groat, and three Groat make a Shilling, and five Shillings make a Pou- oh, you get the point...\n",
    "---\n",
    "## TLDR, this section is about money\n",
    "\n",
    "Sean_Bean_Money.mp3. Anyway, money is being handled differently because it is not explicitly tagged in the XML file like defendants or offences are. So, instead of reading structured <rs> or <persName> elements, we must generate code that searches the raw text files for patterns like \"26 s\" or \"6 d\". Basically, we need to perform a bit of regex. Which does open us up to errors, but we don't have to be absolutely perfect with the code either. \n",
    "\n",
    "Now, we can't just pull from all the text as then we'd have the issue of including things like fines, wages, etc. What we want is the stolen value and the fined value only. Luckily, Old Bailey contains stolen value in the OffenceDescription element, so we can search specifically there to filter out all the other money nonsense.\n",
    "\n",
    "Time to parse that free text of the Old Bailey and see if we can get this to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de0884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first thing we need to do is find what the Old Bailey actually uses for currency. We don't any curveballs like marks or something. Or Groats. \n",
    "\n",
    "what_currencies = Counter()\n",
    "\n",
    "for filepath in da_xml_files:\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    text = ''.join(root.itertext())\n",
    "    \n",
    "    strike_matches = re.findall(r'\\d+\\s*([a-zA-Z]{1,10})\\.', text)\n",
    "    for m in strike_matches:\n",
    "        what_currencies[m.lower()] += 1\n",
    "\n",
    "print(\"Most common patterns (number followed by letters then period):\")\n",
    "for pattern, count in what_currencies.most_common(50):\n",
    "    print(f\"  {pattern}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57da516",
   "metadata": {},
   "source": [
    "Aha! As we can see, our most common ones are shillings, pounds, pence, and guineas. Now, given the \"offenceDescription\" element only contains monetary reference to stealing, we can thus make code to count this up. Note however, that someone may steal money AND an item valued at a certain amount. So, we need to add that together into a total monetary value that was stolen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wheres_the_money_lebowski(trial_elem):\n",
    "    # First, we need to combine all the text inside the offenceDescription tags so we can search it\n",
    "    text = \"\"\n",
    "    for rs in trial_elem.iter('rs'):\n",
    "        if rs.get('type') == 'offenceDescription':\n",
    "            text += ' ' + ''.join(rs.itertext())\n",
    "    \n",
    "    pounds = 0 \n",
    "    shillings = 0\n",
    "    pence = 0\n",
    "    guineas = 0 \n",
    "    # We are using integers now so we can add multiple values\n",
    "    \n",
    "    # Pound search \n",
    "    pound_matches = re.findall(r'(\\d+)\\s*l\\.', text) # Note that l. (from Latin's libra) is the old abbreviation for Pounds, not the Â£ we know and love.\n",
    "    for match in pound_matches:\n",
    "        pounds += int(match)\n",
    "    \n",
    "    pound_word_matches = re.findall(r'(\\d+)\\s*pounds?', text, re.IGNORECASE) # Catch pounds in words. \n",
    "    for match in pound_word_matches:\n",
    "        pounds += int(match)\n",
    "    \n",
    "    # Shilling search \n",
    "    shilling_matches = re.findall(r'(\\d+)\\s*s\\.', text) # Shilling is s. (from Latin's solidus)\n",
    "    for match in shilling_matches:\n",
    "        shillings += int(match)\n",
    "    \n",
    "    shilling_word_matches = re.findall(r'(\\d+)\\s*shillings?', text, re.IGNORECASE) \n",
    "    for match in shilling_word_matches:\n",
    "        shillings += int(match)\n",
    "    \n",
    "    # Pence search \n",
    "    pence_matches = re.findall(r'(\\d+)\\s*d\\.', text) # Pence is d. (from Latin's denarius)\n",
    "    for match in pence_matches:\n",
    "        pence += int(match)\n",
    "    \n",
    "    pence_word_matches = re.findall(r'(\\d+)\\s*pence', text, re.IGNORECASE) # Make sure we also catch things like \"6 pence\"\n",
    "    for match in pence_word_matches:\n",
    "        pence += int(match)\n",
    "    \n",
    "    # Guinea search\n",
    "    guinea_matches = re.findall(r'(\\d+)\\s*guineas?', text, re.IGNORECASE)\n",
    "    for match in guinea_matches:\n",
    "        guineas += int(match)\n",
    "    \n",
    "    return pounds, shillings, pence, guineas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b00d9",
   "metadata": {},
   "source": [
    "Let's do a quick test to see if this shambolic attempt at code actually works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf970b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(input_dir / '16931012.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "count = 0\n",
    "for div1 in root.iter('div1'):\n",
    "    if div1.get('type') == 'trialAccount':\n",
    "        trial_id = div1.get('id')\n",
    "        pounds, shillings, pence, guineas = wheres_the_money_lebowski(div1)\n",
    "        if pounds or shillings or pence or guineas:\n",
    "            print(f\"{trial_id}: {pounds} pounds, {shillings} shillings, {pence} pence, {guineas} guineas\")\n",
    "            count += 1\n",
    "            if count >= 10:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21165061",
   "metadata": {},
   "source": [
    "Alright! It worked. Now we can do the same for fines, which luckily is only in the \"punishmentDescription\" tag. In fact, fines can only be found in the subcategory to the 'fine' category in general, so it is really easy for us to get those numbers. On the flip side, they spell the word \"value\" incorrectly multiple times, so we should avoid a regex with that word.\n",
    "\n",
    "We also are going to include \"Marks\" for fines, which was a non-codified unit of currency in Britain. I saw it in one or two fines so we may as well be certain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e2b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def damn_you_fine(trial_elem):\n",
    "    # Search only within punishmentDescription tags that are specifically fines. \n",
    "    # Very similar to how we did before with the <rs> element. \n",
    "\n",
    "    text = \"\"\n",
    "    for rs in trial_elem.iter('rs'):\n",
    "        if rs.get('type') == 'punishmentDescription':\n",
    "            # Check if this is a fine\n",
    "            for interp in rs.iter('interp'):\n",
    "                if interp.get('type') == 'punishmentSubcategory' and interp.get('value') == 'fine':\n",
    "                    text += ' ' + ''.join(rs.itertext())\n",
    "                    break\n",
    "    \n",
    "    fine_pounds = 0\n",
    "    fine_shillings = 0\n",
    "    fine_pence = 0\n",
    "    fine_guineas = 0\n",
    "    fine_marks = 0\n",
    "    \n",
    "    # Pounds \n",
    "    for match in re.findall(r'(\\d+)\\s*l\\.', text):\n",
    "        fine_pounds += int(match)\n",
    "    for match in re.findall(r'(\\d+)\\s*pounds?', text, re.IGNORECASE):\n",
    "        fine_pounds += int(match)\n",
    "    \n",
    "    # Shillings\n",
    "    for match in re.findall(r'(\\d+)\\s*s\\.', text):\n",
    "        fine_shillings += int(match)\n",
    "    for match in re.findall(r'(\\d+)\\s*shillings?', text, re.IGNORECASE):\n",
    "        fine_shillings += int(match)\n",
    "    \n",
    "    # Pence \n",
    "    for match in re.findall(r'(\\d+)\\s*d\\.', text):\n",
    "        fine_pence += int(match)\n",
    "    for match in re.findall(r'(\\d+)\\s*pence', text, re.IGNORECASE):\n",
    "        fine_pence += int(match)\n",
    "    \n",
    "    # Guineas\n",
    "    for match in re.findall(r'(\\d+)\\s*guineas?', text, re.IGNORECASE):\n",
    "        fine_guineas += int(match)\n",
    "    \n",
    "    # Marks\n",
    "    for match in re.findall(r'(\\d+)\\s*marke?s?', text, re.IGNORECASE):\n",
    "        fine_marks += int(match)\n",
    "    \n",
    "    return fine_pounds, fine_shillings, fine_pence, fine_guineas, fine_marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f06aff",
   "metadata": {},
   "source": [
    "And a quick test code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511075c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for filepath in da_xml_files:\n",
    "    if count >= 15:\n",
    "        break\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for div1 in root.iter('div1'):\n",
    "        if count >= 15:\n",
    "            break\n",
    "        if div1.get('type') == 'trialAccount':\n",
    "            pounds, shillings, pence, guineas, marks = damn_you_fine(div1)\n",
    "            if pounds or shillings or pence or guineas or marks:\n",
    "                print(f\"{trial_id}: {pounds} pounds, {shillings} shillings, {pence} pence, {guineas} guineas, {marks} marks\")\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016dd022",
   "metadata": {},
   "source": [
    "Awesome, awesome! We got that all to work perfectly! Now, let's move on. \n",
    "\n",
    "## Code of Hammurabi or something, idk I didn't watch the movie\n",
    "\n",
    "Right, now onto our last issue. Imprisonment time isn't something that can be directly extracted either. So, we will need to do that similarly to the money amounts in order to see for how LONG someone was imprisoned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cd4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_HUNDRED_life_sentences(trial_elem):\n",
    "    text = \"\"\n",
    "    for rs in trial_elem.iter('rs'): # This is very similar to before. We are searching the <rs> element for the imprison subcategory\n",
    "        if rs.get('type') == 'punishmentDescription':\n",
    "            for interp in rs.iter('interp'):\n",
    "                if interp.get('type') == 'punishmentCategory' and interp.get('value') == 'imprison':\n",
    "                    text += ' ' + ''.join(rs.itertext()) # This will add the full text of our <rs> element to our text string, meaning we can analyse it.\n",
    "                    # I might have already explained what that line does in a previous function but uh\n",
    "                    # My memory is going ngl\n",
    "                    break\n",
    "    \n",
    "    # Some common misspellings I found in the XML files. We are likely to miss some. \n",
    "    # More can be added if we discover more. \n",
    "    minor_spelling_mistake = {\n",
    "        'tweleve': 'twelve',\n",
    "        'bight': 'eight',\n",
    "        'pour': 'four'\n",
    "    }\n",
    "    \n",
    "    # I ran some code as to find the words which provide the most noise. \n",
    "    # This will allow us to exclude them. \n",
    "    # As with the spelling mistakes, we can add more should it become an issue. But given we have so MANY trials and so little errors\n",
    "    # We shouldn't need to worry\n",
    "    noise = ['last', 'calendar', 'calender', 'each', 'first', 'there', 'sir', 'fine', 'the', 'a', 'an']\n",
    "    \n",
    "\n",
    "    # First we find numbers that are followed by a time unit, making sure we actually ignore case specific language\n",
    "    match = re.search(r'([a-zA-Z\\-]+|\\d+)\\s*(month|year|week|day)s?', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        num_str = match.group(1).lower()\n",
    "        unit = match.group(2).lower() # If we find one, we extract the number and the unit and convert them both to lower case\n",
    "        \n",
    "        # This will fix the three common spelling mistakes we found. \n",
    "        if num_str in minor_spelling_mistake:\n",
    "            num_str = minor_spelling_mistake[num_str] \n",
    "        \n",
    "        # Skip the common noise words, we don't need them\n",
    "        if num_str in noise:\n",
    "            return \"\"\n",
    "        \n",
    "        # Now we use the word2number import to convert them and bam. We done. \n",
    "        try:\n",
    "            if num_str.isdigit():\n",
    "                num = int(num_str)\n",
    "            else:\n",
    "                num = w2n.word_to_num(num_str)\n",
    "            return f\"{num} {unit}s\"\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a14be8",
   "metadata": {},
   "source": [
    "It's the FINAL test. Hell yeah, I am sick of writing these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for filepath in da_xml_files:\n",
    "    if count >= 15:\n",
    "        break\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for div1 in root.iter('div1'):\n",
    "        if count >= 15:\n",
    "            break\n",
    "        if div1.get('type') == 'trialAccount':\n",
    "            duration = five_HUNDRED_life_sentences(div1)\n",
    "            if duration:\n",
    "                print(f\"{filepath.name} - {div1.get('id')}: {duration}\")\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31064e8",
   "metadata": {},
   "source": [
    "It WORKS. Let's gooooooooooooo. \n",
    "\n",
    "Now, let's bring it all together into one big place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe54116",
   "metadata": {},
   "source": [
    "## Bringing It All Together\n",
    "\n",
    "We are at the end of our XML journey, sorry kids. But it was quite fun, if not finnicky. Now, lets create one last function that takes everything we've done and uses all our tested functions. After that, we shall convert it to a csv and actually perform analysis on it. Further data cleaning will have to be performed on an adhoc basis (for example, fixing case elements or adding something like the population of London for each year). This also includes things like converting to date-time, if needed. The main function of this portion was just extracting the data correctly from the XML files.\n",
    "\n",
    "Anyway, let us commence forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79185e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shayb\\AppData\\Local\\Temp\\ipykernel_6744\\226569130.py:1: DtypeWarning: Columns (6,15,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('old_bailey_actual_final.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_date</th>\n",
       "      <th>session_year</th>\n",
       "      <th>trial_id</th>\n",
       "      <th>defendant_name</th>\n",
       "      <th>defendant_gender</th>\n",
       "      <th>defendant_age</th>\n",
       "      <th>defendant_occupation</th>\n",
       "      <th>victim_gender</th>\n",
       "      <th>offence_category</th>\n",
       "      <th>offence_subcategory</th>\n",
       "      <th>...</th>\n",
       "      <th>offence_value_shillings</th>\n",
       "      <th>offence_value_pence</th>\n",
       "      <th>offence_value_guineas</th>\n",
       "      <th>fine_value_pounds</th>\n",
       "      <th>fine_value_shillings</th>\n",
       "      <th>fine_value_pence</th>\n",
       "      <th>fine_value_guineas</th>\n",
       "      <th>fine_value_marks</th>\n",
       "      <th>juror_ids</th>\n",
       "      <th>judge_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16740429</td>\n",
       "      <td>1674</td>\n",
       "      <td>t16740429-1</td>\n",
       "      <td>Prisoner</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>violentTheft</td>\n",
       "      <td>highwayRobbery</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16740429</td>\n",
       "      <td>1674</td>\n",
       "      <td>t16740429-2</td>\n",
       "      <td>another</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>theft</td>\n",
       "      <td>grandLarceny</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16740429</td>\n",
       "      <td>1674</td>\n",
       "      <td>t16740429-3</td>\n",
       "      <td>others</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>theft</td>\n",
       "      <td>burglary</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16740429</td>\n",
       "      <td>1674</td>\n",
       "      <td>t16740429-3</td>\n",
       "      <td>one</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>theft</td>\n",
       "      <td>burglary</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16740429</td>\n",
       "      <td>1674</td>\n",
       "      <td>t16740429-3</td>\n",
       "      <td>more</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>theft</td>\n",
       "      <td>burglary</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_date  session_year     trial_id defendant_name defendant_gender  \\\n",
       "0      16740429          1674  t16740429-1       Prisoner             male   \n",
       "1      16740429          1674  t16740429-2        another             male   \n",
       "2      16740429          1674  t16740429-3         others             male   \n",
       "3      16740429          1674  t16740429-3            one             male   \n",
       "4      16740429          1674  t16740429-3           more             male   \n",
       "\n",
       "  defendant_age defendant_occupation victim_gender offence_category  \\\n",
       "0           NaN                  NaN          male     violentTheft   \n",
       "1           NaN                  NaN          male            theft   \n",
       "2           NaN                  NaN          male            theft   \n",
       "3           NaN                  NaN          male            theft   \n",
       "4           NaN                  NaN          male            theft   \n",
       "\n",
       "  offence_subcategory  ... offence_value_shillings offence_value_pence  \\\n",
       "0      highwayRobbery  ...                       0                   0   \n",
       "1        grandLarceny  ...                       0                   0   \n",
       "2            burglary  ...                       0                   0   \n",
       "3            burglary  ...                       0                   0   \n",
       "4            burglary  ...                       0                   0   \n",
       "\n",
       "  offence_value_guineas fine_value_pounds fine_value_shillings  \\\n",
       "0                     0                 0                    0   \n",
       "1                     0                 0                    0   \n",
       "2                     0                 0                    0   \n",
       "3                     0                 0                    0   \n",
       "4                     0                 0                    0   \n",
       "\n",
       "  fine_value_pence fine_value_guineas  fine_value_marks  juror_ids  judge_ids  \n",
       "0                0                  0                 0        NaN        NaN  \n",
       "1                0                  0                 0        NaN        NaN  \n",
       "2                0                  0                 0        NaN        NaN  \n",
       "3                0                  0                 0        NaN        NaN  \n",
       "4                0                  0                 0        NaN        NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('old_bailey_actual_final.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8162c42",
   "metadata": {},
   "source": [
    "## what do we have here? \n",
    "(In bold are the most useful columns for our research)\n",
    "\n",
    "Here we have the old bailey proceedings, where each row represents a trial. \n",
    "- in the session_date column we have the date of that trial in YYYY/MM/DD format - which needs to be converted to datetime format it it is to be used\n",
    "- **In session_year we have the year**\n",
    "- in trial_id, each trial has a unique ID number. \n",
    "- in defendant_name we have some traditional names and some given names like \"prisoner\". this is the person who is being trialed.\n",
    "- **in defendant_gender we have their gender which is either male, female or indeterminate.**\n",
    "- **in defendant occupation we have a mix of job roles**\n",
    "- **in victim_gender we have the gender of the victim of the crime, if the crime was comitted against someone**\n",
    "- offence_category contains 9 unique values that are overarching categories of crime. \n",
    "- **offence_subtagories are more specific, containing over 50 types of crime which all fall into a large category of crime.**\n",
    "- **the punishment column contains 6 different punishment types containing NaN, death, transport, miscpunish, corporal, nopunish, imprison.**\n",
    "- **punishment_detail contains more specific punishment types like publicwhipping, hardlabour, insanity, branding, pardoned etc.**\n",
    "-  in crime location we have some areas of london listed, but only 15 values listed. \n",
    "- value_pounds, shillings and pence with the highest value in all 3 columns is 69712 shillings, 4000 pounds and 500 pence.\n",
    "- fine_pounds contains floats, with the price in pounds that the defendant was fined for the crime.\n",
    "- Juror and judge Id's contain the relative juror and judge unique identifiers for that case. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9e753",
   "metadata": {},
   "source": [
    "# Planning: Data Science Life Cycle\n",
    "Any good Data Science project starts with a good plan. To do this we need to start by understanding the data science lifecycle. The lifecycle below has been adapted slightly to fit an LIS project rather than a data science business project. See column 'How we will do this' to see a summary about how we completed every step of the life cycle in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73f07df",
   "metadata": {},
   "source": [
    "| Phases | Description | How we will do this |\n",
    "|--------|-------------|---------------------|\n",
    "| Identifying problems and understanding the project | Discovering the answers for basic questions including requirements, priorities and budget of the project. | As a group we evaluated our collective interests and aligned these with the assessment brief to decide on the project topic that would fit best. |\n",
    "| Data Collection | Collecting data from relevant sources either in structured or unstructured form. | Using our collective interests in crime, gender inequality and historic datasets, we found a good dataset - The Old Bailey files. We then took our time to fully understand the data by exploring the XML files and the potential of the dataset. |\n",
    "| Data processing | Processing and fine-tuning the raw data, critical for the goodness of the overall project. | AKA cleaning the data. We did this as shown above by merging XML files and extracting tags to create a pandas dataframe, and create a new CSV file which is used from here onwards.|\n",
    "| Data modelling | Preparing the appropriate model to achieve desired performance. | Actually doing the analysis, starting below. In order to do this, we needed to plan what we wanted to find out by generating some hypotheses. See hypotheses below.|\n",
    "| Visualisation | Creating clear, professional data visualisations to support analysis. | Using seaborn and plotly visualisations, including a Shiny app.py |\n",
    "| Model deployment | Executing the analysed model in desired format and channel.| Shiny App. |\n",
    "| Limitations & Future Work | Acknowledging constraints and proposing extensions. | critical analysis of our research, findings and methods, in written format as a markdown |\n",
    "\n",
    "source: https://www.onlinemanipal.com/blogs/data-science-lifecycle-explained\n",
    "# Analysing\n",
    "\n",
    "## Hypotheses: \n",
    "\n",
    "1. Female defendants were less likely to receive death or corporal punishment compared to male defendants\n",
    "\n",
    "2. Defendants gender and occupation can be used to predict punishment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cf4465",
   "metadata": {},
   "source": [
    "### Lets begin:\n",
    "Female defendants were less likely to receive death or corporal punishment compared to male defendants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f68806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'respitedForPregnancy', 'drawnAndQuartered', 'burning',\n",
       "       'branding', 'fine', 'publicWhipping', 'sureties', 'pillory',\n",
       "       'whipping', 'pardon', 'executed', 'houseOfCorrection', 'newgate',\n",
       "       'sentenceRespited', 'respited', 'privateWhipping',\n",
       "       'militaryNavalDuty', 'forfeiture', 'brandingOnCheek',\n",
       "       'hangingInChains', 'hardLabour', 'deathAndDissection', 'insanity',\n",
       "       'otherInstitution', 'no_subcategory', 'penalServitude',\n",
       "       'preventiveDetention'], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what kinds of punishments are there in this dataset and which ones are we going to categorise as harsh\n",
    "df['punishment_detail'].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d4990af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define harsh punishments\n",
    "harsh = ['executed', 'drawnAndQuartered', 'burning', 'branding', 'hangingInChains', \n",
    "         'deathAndDissection', 'hardLabour', 'publicWhipping', 'privateWhipping', 'whipping', 'pillory']\n",
    "\n",
    "medium = ['fine', 'sureties', 'forfeiture', 'houseOfCorrection', 'newgate', 'penalServitude', \n",
    "          'preventiveDetention', 'militaryNavalDuty', 'sentenceRespited', 'respited']\n",
    "\n",
    "lenient = ['pardon', 'respitedForPregnancy', 'insanity', 'no_subcategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b90dcb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by gender\n",
    "male = df[df['defendant_gender'] == 'male']\n",
    "female= df[df['defendant_gender'] == 'female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f0bb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate proportion receiving harsh punishment\n",
    "male_harsh = (male['punishment_detail'].isin(harsh)).sum() / len(male)\n",
    "female_harsh = (female['punishment_detail'].isin(harsh)).sum() / len(female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a3dabff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male defendants harsh punishment: 0.13481242360314696\n",
      "Female defendants harsh punishment: 0.08877213332878442\n",
      "Difference: 0.046040290274362544\n"
     ]
    }
   ],
   "source": [
    "print(\"Male defendants harsh punishment:\", male_harsh)\n",
    "print(\"Female defendants harsh punishment:\", female_harsh)\n",
    "print(\"Difference:\", male_harsh - female_harsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6c99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square: 890.4867690650723\n",
      "P-value: 1.1478760025674457e-195\n",
      "Significant? Yes\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "contingency_table = [\n",
    "    [(male['punishment_detail'].isin(harsh)).sum(), len(male) - (male['punishment_detail'].isin(harsh)).sum()],\n",
    "    [(female['punishment_detail'].isin(harsh)).sum(), len(female) - (female['punishment_detail'].isin(harsh)).sum()]\n",
    "]\n",
    "\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(\"Chi-square:\", chi2)\n",
    "print(\"P-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6824eb97",
   "metadata": {},
   "source": [
    "## Hypothesis: Gender Differences in Harsh Sentencing\n",
    "\n",
    "**Hypothesis:** Female defendants were less likely to receive harsh punishments compared to male defendants.\n",
    "\n",
    "**Method:**\n",
    "\n",
    "We compared the proportion of male and female defendants who received harsh punishments, defined as: executed, drawnAndQuartered, burning, branding, hangingInChains, deathAndDissection, hardLabour, publicWhipping, privateWhipping, whipping, and pillory.\n",
    "\n",
    "**Results:**\n",
    "\n",
    "- Male defendants: 13.48% received harsh punishment\n",
    "- Female defendants: 8.88% received harsh punishment\n",
    "- Difference: 4.60 percentage points\n",
    "\n",
    "A chi-square test confirmed this difference is highly statistically significant (ÏÂ² = 890.49, p < 0.001).\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Female defendants were significantly less likely to receive harsh physical punishments compared to male defendants. This represents a 4.60 percentage point difference, with females receiving harsh punishments at roughly two-thirds the rate of males. This suggests that courts applied more lenient sentencing practices to women overall. \n",
    "\n",
    "however now we need to test to see if female defendants recieved lighter punishments than men for the **same types of crime.**\n",
    "\n",
    "### Punishments for the same types of crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d0e5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# when crime # X, harsh punishments are given to men more than women\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Test for theft\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m male_theft = \u001b[43mdf\u001b[49m[(df[\u001b[33m'\u001b[39m\u001b[33mdefendant_gender\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mmale\u001b[39m\u001b[33m'\u001b[39m) & (df[\u001b[33m'\u001b[39m\u001b[33moffence_category\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mtheft\u001b[39m\u001b[33m'\u001b[39m)]\n\u001b[32m      4\u001b[39m female_theft = df[(df[\u001b[33m'\u001b[39m\u001b[33mdefendant_gender\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mfemale\u001b[39m\u001b[33m'\u001b[39m) & (df[\u001b[33m'\u001b[39m\u001b[33moffence_category\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mtheft\u001b[39m\u001b[33m'\u001b[39m)]\n\u001b[32m      6\u001b[39m male_theft_harsh = (male_theft[\u001b[33m'\u001b[39m\u001b[33mpunishment_detail\u001b[39m\u001b[33m'\u001b[39m].isin(harsh)).sum() / \u001b[38;5;28mlen\u001b[39m(male_theft)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# when crime # X, harsh punishments are given to men more than women\n",
    "# Test for theft\n",
    "male_theft = df[(df['defendant_gender'] == 'male') & (df['offence_category'] == 'theft')]\n",
    "female_theft = df[(df['defendant_gender'] == 'female') & (df['offence_category'] == 'theft')]\n",
    "\n",
    "male_theft_harsh = (male_theft['punishment_detail'].isin(harsh)).sum() / len(male_theft)\n",
    "female_theft_harsh = (female_theft['punishment_detail'].isin(harsh)).sum() / len(female_theft)\n",
    "\n",
    "print(\"Theft - Male harsh:\", male_theft_harsh)\n",
    "print(\"Theft - Female harsh:\", female_theft_harsh)\n",
    "\n",
    "# Test for murder\n",
    "male_murder = df[(df['defendant_gender'] == 'male') & (df['offence_category'] == 'murder')]\n",
    "female_murder = df[(df['defendant_gender'] == 'female') & (df['offence_category'] == 'murder')]\n",
    "\n",
    "male_murder_harsh = (male_murder['punishment_detail'].isin(harsh)).sum() / len(male_murder)\n",
    "female_murder_harsh = (female_murder['punishment_detail'].isin(harsh)).sum() / len(female_murder)\n",
    "\n",
    "print(\"Murder - Male harsh:\", male_murder_harsh)\n",
    "print(\"Murder - Female harsh:\", female_murder_harsh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64911aa6",
   "metadata": {},
   "source": [
    " analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ffee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis 2. 1870 hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ffa24",
   "metadata": {},
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc43c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression to answer hypothesis 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bccb6c",
   "metadata": {},
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e59d8",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "using visualisation tools to map our findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4b6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccb2c986",
   "metadata": {},
   "source": [
    "# evaluating\n",
    "\n",
    "What patterns can we see here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d5a18c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ce124ff",
   "metadata": {},
   "source": [
    "# Visualising\n",
    "\n",
    "Now we present our findings in Shiny!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf54bfc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
